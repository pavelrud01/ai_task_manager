"""
–£—Ç–∏–ª–∏—Ç—ã –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è —á–µ–ª–æ–≤–µ–∫–æ-—á–∏—Ç–∞–µ–º—ã—Ö —ç–∫—Å–ø–æ—Ä—Ç–æ–≤ –∞—Ä—Ç–µ—Ñ–∞–∫—Ç–æ–≤
"""

import json
from pathlib import Path
from typing import Dict, Any, List
from datetime import datetime


def export_human_readable(
    artifact_data: Dict[str, Any],
    step_name: str,
    output_dir: Path,
    metadata: Dict[str, Any] = None
) -> Path:
    """
    –°–æ–∑–¥–∞–µ—Ç —á–µ–ª–æ–≤–µ–∫–æ-—á–∏—Ç–∞–µ–º—É—é –≤–µ—Ä—Å–∏—é –∞—Ä—Ç–µ—Ñ–∞–∫—Ç–∞.
    
    Args:
        artifact_data: –î–∞–Ω–Ω—ã–µ –∞—Ä—Ç–µ—Ñ–∞–∫—Ç–∞
        step_name: –ò–º—è —à–∞–≥–∞
        output_dir: –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è
        metadata: –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
    
    Returns:
        –ü—É—Ç—å –∫ —Å–æ–∑–¥–∞–Ω–Ω–æ–º—É —Ñ–∞–π–ª—É
    """
    output_dir.mkdir(parents=True, exist_ok=True)
    
    # –°–æ–∑–¥–∞–µ–º –∏–º—è —Ñ–∞–π–ª–∞
    filename = f"{step_name}_HUMAN_READABLE.md"
    output_path = output_dir / filename
    
    # –ò–∑–≤–ª–µ–∫–∞–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –∏–∑ –∞—Ä—Ç–µ—Ñ–∞–∫—Ç–∞
    artifact_metadata = artifact_data.get("__metadata", {})
    
    # –°–æ–∑–¥–∞–µ–º markdown –∫–æ–Ω—Ç–µ–Ω—Ç
    content = _generate_human_readable_content(
        artifact_data, step_name, artifact_metadata, metadata
    )
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ñ–∞–π–ª
    output_path.write_text(content, encoding='utf-8')
    
    return output_path


def _generate_human_readable_content(
    data: Dict[str, Any],
    step_name: str,
    artifact_metadata: Dict[str, Any],
    additional_metadata: Dict[str, Any] = None
) -> str:
    """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ human-readable —Ñ–∞–π–ª–∞."""
    
    # –ó–∞–≥–æ–ª–æ–≤–æ–∫
    content = [f"# {step_name.upper()} - Human Readable Export"]
    content.append("")
    
    # –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
    content.append("## üìã Metadata")
    content.append("")
    
    # –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –∏–∑ –∞—Ä—Ç–µ—Ñ–∞–∫—Ç–∞
    if artifact_metadata:
        content.append("### Standard Information")
        content.append(f"- **Standard ID**: {artifact_metadata.get('standard_id', 'N/A')}")
        content.append(f"- **Source Path**: {artifact_metadata.get('source_path', 'N/A')}")
        content.append(f"- **Fingerprint**: `{artifact_metadata.get('fingerprint', 'N/A')[:16]}...`")
        content.append(f"- **Timestamp**: {artifact_metadata.get('timestamp', 'N/A')}")
        content.append("")
    
    # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
    if additional_metadata:
        content.append("### Execution Information")
        for key, value in additional_metadata.items():
            content.append(f"- **{key.replace('_', ' ').title()}**: {value}")
        content.append("")
    
    # –°–≤–æ–¥–∫–∞ –¥–∞–Ω–Ω—ã—Ö
    content.append("## üìä Data Summary")
    content.append("")
    
    # –ü–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ–º –ø–æ–ª—è
    data_fields = [k for k in data.keys() if not k.startswith('__')]
    content.append(f"- **Total Fields**: {len(data_fields)}")
    content.append(f"- **Data Keys**: {', '.join(data_fields)}")
    content.append("")
    
    # –°–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã–π –∫–æ–Ω—Ç–µ–Ω—Ç –¥–ª—è —Ä–∞–∑–Ω—ã—Ö —Ç–∏–ø–æ–≤ —à–∞–≥–æ–≤
    if step_name.startswith("step_03"):
        content.extend(_format_interview_content(data))
    elif step_name.startswith("step_04"):
        content.extend(_format_jtbd_content(data))
    elif step_name.startswith("step_05"):
        content.extend(_format_segments_content(data))
    elif step_name.startswith("step_06"):
        content.extend(_format_decision_mapping_content(data))
    else:
        content.extend(_format_generic_content(data))
    
    # –ü–æ–ª–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
    content.append("## üîç Full Data (JSON)")
    content.append("")
    content.append("```json")
    content.append(json.dumps(data, indent=2, ensure_ascii=False))
    content.append("```")
    content.append("")
    
    # –§—É—Ç–µ—Ä
    content.append("---")
    content.append("*Generated by AI Marketing Agent*")
    content.append(f"*Timestamp: {datetime.now().isoformat()}*")
    
    return "\n".join(content)


def _format_interview_content(data: Dict[str, Any]) -> List[str]:
    """–§–æ—Ä–º–∞—Ç–∏—Ä—É–µ—Ç –∫–æ–Ω—Ç–µ–Ω—Ç –¥–ª—è –∏–Ω—Ç–µ—Ä–≤—å—é."""
    content = []
    
    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –∏–Ω—Ç–µ—Ä–≤—å—é
    interviews = data.get("interviews", [])
    if interviews:
        content.append("### Interview Statistics")
        content.append(f"- **Total Interviews**: {len(interviews)}")
        
        # –ì—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∞ –ø–æ –ø–µ—Ä—Å–æ–Ω–∞–º
        personas = {}
        for interview in interviews:
            persona = interview.get("persona", "Unknown")
            personas[persona] = personas.get(persona, 0) + 1
        
        content.append("- **Personas Distribution**:")
        for persona, count in personas.items():
            content.append(f"  - {persona}: {count} interviews")
        content.append("")
    
    return content


def _format_jtbd_content(data: Dict[str, Any]) -> List[str]:
    """–§–æ—Ä–º–∞—Ç–∏—Ä—É–µ—Ç –∫–æ–Ω—Ç–µ–Ω—Ç –¥–ª—è JTBD."""
    content = []
    
    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ JTBD
    jobs = data.get("jobs", [])
    if jobs:
        content.append("### JTBD Statistics")
        content.append(f"- **Total Jobs**: {len(jobs)}")
        
        # –ì—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∞ –ø–æ —Ç–∏–ø–∞–º
        job_types = {}
        for job in jobs:
            job_type = job.get("type", "Unknown")
            job_types[job_type] = job_types.get(job_type, 0) + 1
        
        content.append("- **Job Types Distribution**:")
        for job_type, count in job_types.items():
            content.append(f"  - {job_type}: {count} jobs")
        content.append("")
    
    return content


def _format_segments_content(data: Dict[str, Any]) -> List[str]:
    """–§–æ—Ä–º–∞—Ç–∏—Ä—É–µ—Ç –∫–æ–Ω—Ç–µ–Ω—Ç –¥–ª—è —Å–µ–≥–º–µ–Ω—Ç–æ–≤."""
    content = []
    
    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —Å–µ–≥–º–µ–Ω—Ç–æ–≤
    segments = data.get("segments", [])
    if segments:
        content.append("### Segments Statistics")
        content.append(f"- **Total Segments**: {len(segments)}")
        
        # –ì—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∞ –ø–æ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç—É
        priorities = {}
        for segment in segments:
            priority = segment.get("priority", "Unknown")
            priorities[priority] = priorities.get(priority, 0) + 1
        
        content.append("- **Priority Distribution**:")
        for priority, count in priorities.items():
            content.append(f"  - Priority {priority}: {count} segments")
        content.append("")
    
    return content


def _format_decision_mapping_content(data: Dict[str, Any]) -> List[str]:
    """–§–æ—Ä–º–∞—Ç–∏—Ä—É–µ—Ç –∫–æ–Ω—Ç–µ–Ω—Ç –¥–ª—è –∫–∞—Ä—Ç —Ä–µ—à–µ–Ω–∏–π."""
    content = []
    
    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –∫–∞—Ä—Ç —Ä–µ—à–µ–Ω–∏–π
    decision_maps = data.get("decision_maps", [])
    if decision_maps:
        content.append("### Decision Maps Statistics")
        content.append(f"- **Total Decision Maps**: {len(decision_maps)}")
        content.append("")
    
    return content


def _format_generic_content(data: Dict[str, Any]) -> List[str]:
    """–§–æ—Ä–º–∞—Ç–∏—Ä—É–µ—Ç –∫–æ–Ω—Ç–µ–Ω—Ç –¥–ª—è –æ–±—â–∏—Ö —à–∞–≥–æ–≤."""
    content = []
    
    # –û–±—â–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
    content.append("### General Statistics")
    content.append(f"- **Data Structure**: {type(data).__name__}")
    
    # –ü–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ–º –≤–ª–æ–∂–µ–Ω–Ω—ã–µ –æ–±—ä–µ–∫—Ç—ã
    nested_objects = 0
    for value in data.values():
        if isinstance(value, (dict, list)):
            nested_objects += 1
    
    content.append(f"- **Nested Objects**: {nested_objects}")
    content.append("")
    
    return content


def export_multiple_human_readable(
    artifacts: Dict[str, Dict[str, Any]],
    output_dir: Path,
    metadata: Dict[str, Any] = None
) -> List[Path]:
    """
    –°–æ–∑–¥–∞–µ—Ç human-readable –≤–µ—Ä—Å–∏–∏ –¥–ª—è –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –∞—Ä—Ç–µ—Ñ–∞–∫—Ç–æ–≤.
    
    Args:
        artifacts: –°–ª–æ–≤–∞—Ä—å {step_name: artifact_data}
        output_dir: –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è
        metadata: –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
    
    Returns:
        –°–ø–∏—Å–æ–∫ –ø—É—Ç–µ–π –∫ —Å–æ–∑–¥–∞–Ω–Ω—ã–º —Ñ–∞–π–ª–∞–º
    """
    created_files = []
    
    for step_name, artifact_data in artifacts.items():
        try:
            file_path = export_human_readable(
                artifact_data, step_name, output_dir, metadata
            )
            created_files.append(file_path)
        except Exception as e:
            print(f"Error creating human-readable for {step_name}: {e}")
    
    return created_files


def create_human_readable_index(
    created_files: List[Path],
    output_dir: Path
) -> Path:
    """
    –°–æ–∑–¥–∞–µ—Ç –∏–Ω–¥–µ–∫—Å–Ω—ã–π —Ñ–∞–π–ª —Å–æ —Å—Å—ã–ª–∫–∞–º–∏ –Ω–∞ –≤—Å–µ human-readable —ç–∫—Å–ø–æ—Ä—Ç—ã.
    
    Args:
        created_files: –°–ø–∏—Å–æ–∫ –ø—É—Ç–µ–π –∫ —Å–æ–∑–¥–∞–Ω–Ω—ã–º —Ñ–∞–π–ª–∞–º
        output_dir: –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è
    
    Returns:
        –ü—É—Ç—å –∫ –∏–Ω–¥–µ–∫—Å–Ω–æ–º—É —Ñ–∞–π–ª—É
    """
    index_path = output_dir / "HUMAN_READABLE_INDEX.md"
    
    content = ["# Human Readable Exports Index"]
    content.append("")
    content.append(f"Generated: {datetime.now().isoformat()}")
    content.append("")
    content.append("## Available Exports")
    content.append("")
    
    for file_path in sorted(created_files):
        relative_path = file_path.relative_to(output_dir)
        step_name = file_path.stem.replace("_HUMAN_READABLE", "")
        content.append(f"- [{step_name}]({relative_path})")
    
    content.append("")
    content.append("---")
    content.append("*Generated by AI Marketing Agent*")
    
    index_path.write_text("\n".join(content), encoding='utf-8')
    
    return index_path

